import math
import numpy as np
import h5py
import tensorflow as tf
from tensorflow.python.framework import ops

np.random.seed(1)

"""
NB! Datasets exist only in Github repository due to file size capacity in ained.ttu.ee! 
https://github.com/epmaur/traffic-congestion-prediction/tree/master/datasets
"""

def load_dataset():
    dataset = h5py.File('../datasets/optical_and_raw_image_2_layers.hdf5', "r")
    train_set_x_orig = np.array(dataset["train_matrix"][:])  # train set features
    train_set_y_orig = np.array(dataset["train_labels"][:])  # train set labels

    test_set_x_orig = np.array(dataset["test_matrix"][:])  # test set features
    test_set_y_orig = np.array(dataset["test_labels"][:])  # test set labels

    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))
    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))

    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig


def random_mini_batches(X, Y, mini_batch_size=64, seed=0):
    """
    Creates a list of random minibatches from (X, Y)

    Arguments:
    X -- input data, of shape (input size, number of examples) (m, Hi, Wi, Ci)
    Y -- true "label" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples) (m, n_y)
    mini_batch_size - size of the mini-batches, integer

    Returns:
    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)
    """

    m = X.shape[0]
    mini_batches = []
    np.random.seed(seed)

    permutation = list(np.random.permutation(m))
    shuffled_X = X[permutation, :, :, :]
    shuffled_Y = Y[permutation, :]


    num_complete_minibatches = math.floor(
        m / mini_batch_size)
    for k in range(0, num_complete_minibatches):
        mini_batch_X = shuffled_X[k * mini_batch_size: k * mini_batch_size + mini_batch_size, :, :, :]
        mini_batch_Y = shuffled_Y[k * mini_batch_size: k * mini_batch_size + mini_batch_size, :]
        mini_batch = (mini_batch_X, mini_batch_Y)
        mini_batches.append(mini_batch)

    if m % mini_batch_size != 0:
        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size: m, :, :, :]
        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size: m, :]
        mini_batch = (mini_batch_X, mini_batch_Y)
        mini_batches.append(mini_batch)

    return mini_batches


def convert_to_one_hot(Y, C):
    Y = np.eye(C)[Y.reshape(-1)].T
    return Y


def forward_propagation_for_predict(X, parameters):
    """
    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX

    Arguments:
    X -- input dataset placeholder, of shape (input size, number of examples)
    parameters -- python dictionary containing your parameters "W1", "b1", "W2", "b2", "W3", "b3"
                  the shapes are given in initialize_parameters

    Returns:
    Z3 -- the output of the last LINEAR unit
    """

    W1 = parameters['W1']
    b1 = parameters['b1']
    W2 = parameters['W2']
    b2 = parameters['b2']
    W3 = parameters['W3']
    b3 = parameters['b3']

    Z1 = tf.add(tf.matmul(W1, X), b1)
    A1 = tf.nn.relu(Z1)
    Z2 = tf.add(tf.matmul(W2, A1), b2)
    A2 = tf.nn.relu(Z2)
    Z3 = tf.add(tf.matmul(W3, A2), b3)

    return Z3


def create_placeholders(n_H0, n_W0, n_C0, n_y):
    """
   Creates the placeholders for the tensorflow session.

   Arguments:
   n_H0 -- scalar, height of an input image
   n_W0 -- scalar, width of an input image
   n_C0 -- scalar, number of channels of the input
   n_y -- scalar, number of classes

   Returns:
   X -- placeholder for the data input, of shape [None, n_H0, n_W0, n_C0] and dtype "float"
   Y -- placeholder for the input labels, of shape [None, n_y] and dtype "float"
   """
    X = tf.placeholder(tf.float32, shape=[None, n_H0, n_W0, n_C0], name=None)
    Y = tf.placeholder(tf.float32, shape=[None, n_y], name=None)

    return X, Y


def initialize_parameters():
    """
    Initializes weight parameters to build a neural network with tensorflow.
    Returns:
    parameters -- a dictionary of tensors containing W1, W2
    """
    tf.set_random_seed(1)

    W1 = tf.get_variable("W1", [8, 8, 2, 8], initializer=tf.contrib.layers.xavier_initializer(seed=0))
    W2 = tf.get_variable("W2", [2, 2, 8, 16], initializer=tf.contrib.layers.xavier_initializer(seed=0))

    parameters = {"W1": W1,
                  "W2": W2}

    return parameters


def compute_cost(Z3, Y, parameters, lambd, minibatch_size):
    """
    Computes the cost

    Arguments:
    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape
    Y -- "true" labels vector placeholder, same shape as Z3
    parameters -- dict containing W and b values
    lambd -- regularization parameter
    minibatch-size -- size of used minibatch

    Returns:
    cost
    """
    unreg_cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Z3, labels=Y))

    l2_cost = lambd * (1 / minibatch_size) * (
            tf.nn.l2_loss(parameters["W1"]) + tf.nn.l2_loss(parameters["W2"]))
    cost = tf.add(unreg_cost, l2_cost, name='cost')

    return cost


def forward_propagation(X, parameters):
    """
    Implements the forward propagation for the model:
    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED

    Arguments:
    X -- input dataset placeholder, of shape (input size, number of examples)
    parameters -- python dictionary containing your parameters "W1", "W2"
                  the shapes are given in initialize_parameters

    Returns:
    Z3 -- the output of the last LINEAR unit
    """

    W1 = parameters['W1']
    W2 = parameters['W2']

    Z1 = tf.nn.conv2d(X, W1, strides=[1, 1, 1, 1], padding='SAME')
    A1 = tf.nn.relu(Z1)
    P1 = tf.nn.max_pool(A1, ksize=[1, 8, 8, 1], strides=[1, 8, 8, 1], padding='SAME')
    Z2 = tf.nn.conv2d(P1, W2, strides=[1, 1, 1, 1], padding='SAME')
    A2 = tf.nn.relu(Z2)
    P2 = tf.nn.max_pool(A2, ksize=[1, 4, 4, 1], strides=[1, 4, 4, 1], padding='SAME')
    P2 = tf.contrib.layers.flatten(P2)
    Z3 = tf.contrib.layers.fully_connected(P2, 2, activation_fn=None)

    return Z3


def model(X_train, Y_train, X_test, Y_test, learning_rate=0.001,
          num_epochs=5, minibatch_size=30, lambd = 0.0001, print_cost=True):
    """
    Implements a three-layer ConvNet in Tensorflow:
    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED

    Arguments:
    X_train -- training set
    Y_train -- test set
    X_test -- training set
    Y_test -- test set
    learning_rate -- learning rate of the optimization
    num_epochs -- number of epochs of the optimization loop
    minibatch_size -- size of a minibatch
    lambd -- regularization parameter
    print_cost -- True to print the cost every 100 epochs

    Returns:
    train_accuracy -- real number, accuracy on the train set (X_train)
    test_accuracy -- real number, testing accuracy on the test set (X_test)
    parameters -- parameters learnt by the model. They can then be used to predict.
    """
    ops.reset_default_graph()
    tf.set_random_seed(1)  # to keep results consistent (tensorflow seed)
    seed = 3

    (m, n_H0, n_W0, n_C0) = X_train.shape
    n_y = Y_train.shape[1]
    costs = []

    X, Y = create_placeholders(n_H0, n_W0, n_C0, n_y)

    parameters = initialize_parameters()

    Z3 = forward_propagation(X, parameters)
    cost = compute_cost(Z3, Y, parameters, lambd, minibatch_size)
    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)

    init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())

    with tf.Session() as sess:
        sess.run(init)
        for epoch in range(num_epochs):
            minibatch_cost = 0.
            num_minibatches = int(m / minibatch_size)
            seed = seed + 1
            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)

            for minibatch in minibatches:
                (minibatch_X, minibatch_Y) = minibatch

                _, temp_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})

                minibatch_cost += temp_cost / num_minibatches

            # Print the cost every epoch
            if print_cost == True and epoch % 5 == 0:
                print("Cost after epoch %i: %f" % (epoch, minibatch_cost))
            if print_cost == True and epoch % 1 == 0:
                costs.append(minibatch_cost)


        predict_op = tf.argmax(Z3, 1)
        correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))
        accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))
        train_accuracy = accuracy.eval({X: X_train, Y: Y_train})
        test_accuracy = accuracy.eval({X: X_test, Y: Y_test})

        # Analyze results
        correct_labels = tf.argmax(Y, 1)
        evaluated_correct_labels_test = correct_labels.eval({X: X_test, Y: Y_test})
        evaluated_predicted_labels_test = predict_op.eval({X: X_test, Y: Y_test})

        number_of_true_negatives_test = np.where((evaluated_predicted_labels_test == 0) & (evaluated_correct_labels_test == 0))[0].shape
        number_of_true_positives_test = np.where((evaluated_predicted_labels_test == 1) & (evaluated_correct_labels_test == 1))[0].shape
        number_of_false_negatives_test = np.where((evaluated_predicted_labels_test == 0) & (evaluated_correct_labels_test == 1))[0].shape
        number_of_false_positives_test = np.where((evaluated_predicted_labels_test == 1) & (evaluated_correct_labels_test == 0))[0].shape
        print()
        print('True neg test ', number_of_true_negatives_test)
        print('True pos test ', number_of_true_positives_test)
        print('False neg test ', number_of_false_negatives_test)
        print('False pos test ', number_of_false_positives_test)
        print()

        evaluated_correct_labels_train = correct_labels.eval({X: X_train, Y: Y_train})
        evaluated_predicted_labels_train = predict_op.eval({X: X_train, Y: Y_train})
        number_of_true_negatives_train = np.where((evaluated_predicted_labels_train == 0) & (evaluated_correct_labels_train == 0))[0].shape
        number_of_true_positives_train = np.where((evaluated_predicted_labels_train == 1) & (evaluated_correct_labels_train == 1))[0].shape
        number_of_false_negatives_train = np.where((evaluated_predicted_labels_train == 0) & (evaluated_correct_labels_train == 1))[0].shape
        number_of_false_positives_train = np.where((evaluated_predicted_labels_train == 1) & (evaluated_correct_labels_train == 0))[0].shape

        print('True neg train ', number_of_true_negatives_train)
        print('True pos train ', number_of_true_positives_train)
        print('False neg train ', number_of_false_negatives_train)
        print('False pos train ', number_of_false_positives_train)

        sess.run(tf.local_variables_initializer())


        print("Train Accuracy:", train_accuracy)
        print("Test Accuracy:", test_accuracy)

        return train_accuracy, test_accuracy, parameters


X_train_orig, Y_train_orig, X_test_orig, Y_test_orig = load_dataset()

X_train = X_train_orig/255.
X_test = X_test_orig/255.

Y_train = convert_to_one_hot(Y_train_orig, 2).T
Y_test = convert_to_one_hot(Y_test_orig, 2).T
conv_layers = {}

_, _, parameters = model(X_train, Y_train, X_test, Y_test)